{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Non-Negative PARAFAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on [sparse_demo.ipynb](sparse_demo.ipynb#parafac). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we start with a random sparse tensor, constructed so that it has a tensor factorization of rank 5.\n",
    "\n",
    "Because non-negative PARAFAC can take longer to converge than non-masked PARAFAC and also produce dense factors, we will use a smaller tensor than in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<COO: shape=(1000, 5), dtype=float64, nnz=50, fill_value=0.0>,\n",
       " <COO: shape=(1001, 5), dtype=float64, nnz=50, fill_value=0.0>,\n",
       " <COO: shape=(1002, 5), dtype=float64, nnz=50, fill_value=0.0>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (1000, 1001, 1002)\n",
    "rank = 5\n",
    "\n",
    "import sparse\n",
    "starting_weights = sparse.ones(rank)\n",
    "starting_factors = [sparse.random((i, rank)) for i in shape]\n",
    "starting_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th style=\"text-align: left\">Format</th><td style=\"text-align: left\">coo</td></tr><tr><th style=\"text-align: left\">Data Type</th><td style=\"text-align: left\">float64</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(1000, 1001, 1002)</td></tr><tr><th style=\"text-align: left\">nnz</th><td style=\"text-align: left\">4884</td></tr><tr><th style=\"text-align: left\">Density</th><td style=\"text-align: left\">4.869382114891097e-06</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Size</th><td style=\"text-align: left\">152.6K</td></tr><tr><th style=\"text-align: left\">Storage ratio</th><td style=\"text-align: left\">0.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<COO: shape=(1000, 1001, 1002), dtype=float64, nnz=4884, fill_value=0.0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorly.contrib.sparse.cp_tensor import cp_to_tensor\n",
    "tensor = cp_to_tensor((starting_weights, starting_factors))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_size(size_bytes):\n",
    "    size = size_bytes\n",
    "    for unit in ['B', 'KiB', 'MiB', 'GiB', 'TiB']:\n",
    "        if not int(size/1024):\n",
    "            return f'{round(size)}.{unit}'\n",
    "        else:\n",
    "            size /= 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'153.KiB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_size(tensor.nbytes)             # Actual memory usage in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.GiB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "format_size(np.prod(tensor.shape) * 8)  # Memory usage if array was dense, in GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we factor the tensor. Note that at this time, you have to use the `non_negative_parafac` function from the sparse backend when using a sparse mask to avoid memory blowups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%load_ext memory_profiler\n",
    "from tensorly.contrib.sparse.decomposition import non_negative_parafac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction error=0.943207231771627\n",
      "iteration 1, reconstraction error: 0.758341832872278, decrease = 0.18486539889934905\n",
      "iteration 2, reconstraction error: 0.6880274843697167, decrease = 0.07031434850256124\n",
      "iteration 3, reconstraction error: 0.5399027985471302, decrease = 0.14812468582258653\n",
      "iteration 4, reconstraction error: 0.48988338684888716, decrease = 0.05001941169824303\n",
      "iteration 5, reconstraction error: 0.48772494556981577, decrease = 0.002158441279071388\n",
      "iteration 6, reconstraction error: 0.4864308244191075, decrease = 0.0012941211507082606\n",
      "iteration 7, reconstraction error: 0.48555333649559845, decrease = 0.0008774879235090571\n",
      "iteration 8, reconstraction error: 0.48491546799409857, decrease = 0.0006378685014998831\n",
      "iteration 9, reconstraction error: 0.4844427370307753, decrease = 0.0004727309633232868\n",
      "iteration 10, reconstraction error: 0.484098425435411, decrease = 0.00034431159536429945\n",
      "iteration 11, reconstraction error: 0.4838552615479818, decrease = 0.00024316388742917638\n",
      "iteration 12, reconstraction error: 0.48368766295148763, decrease = 0.0001675985964941784\n",
      "iteration 13, reconstraction error: 0.48357286686823314, decrease = 0.00011479608325448698\n",
      "iteration 14, reconstraction error: 0.483493001028683, decrease = 7.986583955016391e-05\n",
      "iteration 15, reconstraction error: 0.4834354548025857, decrease = 5.754622609727145e-05\n",
      "iteration 16, reconstraction error: 0.48339197143721346, decrease = 4.348336537224329e-05\n",
      "iteration 17, reconstraction error: 0.4833573979746129, decrease = 3.4573462600551164e-05\n",
      "iteration 18, reconstraction error: 0.48332861193392107, decrease = 2.8786040691841652e-05\n",
      "iteration 19, reconstraction error: 0.4833037572425588, decrease = 2.4854691362263814e-05\n",
      "iteration 20, reconstraction error: 0.4832817483451167, decrease = 2.2008897442082898e-05\n",
      "iteration 21, reconstraction error: 0.48326196199170146, decrease = 1.9786353415263136e-05\n",
      "iteration 22, reconstraction error: 0.4832440481029167, decrease = 1.7913888784748533e-05\n",
      "iteration 23, reconstraction error: 0.4832278130904759, decrease = 1.623501244080172e-05\n",
      "iteration 24, reconstraction error: 0.48321314714209584, decrease = 1.4665948380065963e-05\n",
      "iteration 25, reconstraction error: 0.48319997892613376, decrease = 1.3168215962089391e-05\n",
      "iteration 26, reconstraction error: 0.4831882481670806, decrease = 1.1730759053141249e-05\n",
      "iteration 27, reconstraction error: 0.4831778903288357, decrease = 1.0357838244934303e-05\n",
      "iteration 28, reconstraction error: 0.48316882960062746, decrease = 9.060728208221125e-06\n",
      "iteration 29, reconstraction error: 0.483160977417892, decrease = 7.852182735468283e-06\n",
      "iteration 30, reconstraction error: 0.4831542343885038, decrease = 6.743029388189825e-06\n",
      "iteration 31, reconstraction error: 0.48314849398786064, decrease = 5.740400643161614e-06\n",
      "iteration 32, reconstraction error: 0.48314364682681005, decrease = 4.847161050591797e-06\n",
      "iteration 33, reconstraction error: 0.48313958469505736, decrease = 4.0621317526912115e-06\n",
      "iteration 34, reconstraction error: 0.48313620392200257, decrease = 3.3807730547841253e-06\n",
      "iteration 35, reconstraction error: 0.48313340786059533, decrease = 2.7960614072397583e-06\n",
      "iteration 36, reconstraction error: 0.48313110848321267, decrease = 2.2993773826640584e-06\n",
      "iteration 37, reconstraction error: 0.48312922718945445, decrease = 1.8812937582213252e-06\n",
      "iteration 38, reconstraction error: 0.4831276949789648, decrease = 1.5322104896520194e-06\n",
      "iteration 39, reconstraction error: 0.48312645215511985, decrease = 1.2428238449491147e-06\n",
      "iteration 40, reconstraction error: 0.4831254477132969, decrease = 1.0044418229382401e-06\n",
      "iteration 41, reconstraction error: 0.4831246385427657, decrease = 8.091705311907837e-07\n",
      "PARAFAC converged after 41 iterations\n",
      "Took 0 mins 15 secs\n",
      "peak memory: 193.16 MiB, increment: 18.25 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "start_time = time.time()\n",
    "sparse_kruskal = non_negative_parafac(tensor, rank=rank, init='random', verbose=True)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print('Took %d mins %d secs' % (divmod(total_time, 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the values that was masked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8, 117, 198])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.coords.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2240418375433051"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_val = tensor[tuple(tensor.coords.T[0])]\n",
    "orig_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [sparse_demo.ipynb](sparse_demo.ipynb) for how to calculate individual values from the factors. Note that we do not compare the entire tensor because it would be dense, and memory usage would be prohibitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.097320102631128e-49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, factors = sparse_kruskal\n",
    "computed_val = np.sum(np.prod(sparse.stack([factors[i][idx] for i, idx in enumerate(tuple(tensor.coords.T[0]))], 0), 0))\n",
    "computed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2240418375433051"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(orig_val - computed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f.density for f in factors]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
