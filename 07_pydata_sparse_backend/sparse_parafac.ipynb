{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parafac\n",
    "\n",
    "Now the CANDECOMP/PARAFAC decomposition. The above tensor is too high a rank to reasonably decompose in this example, so we instead generate an example sparse tensor from a random sparse factorization and re-factor it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sparse\n",
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1000, 1001, 1002, 100)\n",
    "rank = 5\n",
    "\n",
    "starting_factors = [sparse.random((i, rank)) for i in shape]\n",
    "starting_factors\n",
    "starting_weights = sparse.ones(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert it to a tensor. It is very important to use `kruskal_to_tensor` from the sparse backend, as a fully dense version of the tensor would use several TB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th style=\"text-align: left\">Format</th><td style=\"text-align: left\">coo</td></tr><tr><th style=\"text-align: left\">Data Type</th><td style=\"text-align: left\">float64</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(1000, 1001, 1002, 100)</td></tr><tr><th style=\"text-align: left\">nnz</th><td style=\"text-align: left\">4737</td></tr><tr><th style=\"text-align: left\">Density</th><td style=\"text-align: left\">4.722822088091549e-08</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Size</th><td style=\"text-align: left\">185.0K</td></tr><tr><th style=\"text-align: left\">Storage ratio</th><td style=\"text-align: left\">0.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<COO: shape=(1000, 1001, 1002, 100), dtype=float64, nnz=4737, fill_value=0.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorly.contrib.sparse.kruskal_tensor import kruskal_to_tensor\n",
    "tensor = kruskal_to_tensor((starting_weights, starting_factors))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can compare the actual spase used by the tensor vs. what it would require if it were dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00018948"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.nbytes / 1e9                # Actual memory usage in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802.4016"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(tensor.shape) * 8 / 1e9    # Memory usage if array was dense, in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though we started with a sparse tensor, the factors are dense. This is because we used the dense version of `parafac`. Since the factors are in general dense, even for a sparse tensor, this is generally preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose the sparse tensor into a sparse Kruskal tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.contrib.sparse.decomposition import parafac as parafac_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction error=0.5826844593958526\n",
      "iteration 1, reconstruction error: 0.32583862278041187, decrease = 0.2568458366154407, unnormalized = 2.387513538370463\n",
      "iteration 2, reconstruction error: 0.32499296587658, decrease = 0.0008456569038318706, unnormalized = 2.3813171664072916\n",
      "iteration 3, reconstruction error: 0.32497680025263603, decrease = 1.616562394396448e-05, unnormalized = 2.3811987162196093\n",
      "iteration 4, reconstruction error: 0.3249728063814774, decrease = 3.993871158625151e-06, unnormalized = 2.3811694519740745\n",
      "iteration 5, reconstruction error: 0.3249713085966895, decrease = 1.4977847879182882e-06, unnormalized = 2.3811584772730763\n",
      "iteration 6, reconstruction error: 0.3249706074264761, decrease = 7.011702133907782e-07, unnormalized = 2.381153339596754\n",
      "iteration 7, reconstruction error: 0.3249702426319896, decrease = 3.647944865070585e-07, unnormalized = 2.38115066664237\n",
      "iteration 8, reconstruction error: 0.324970039781819, decrease = 2.0285017060528432e-07, unnormalized = 2.381149180300682\n",
      "iteration 9, reconstruction error: 0.3249699217843402, decrease = 1.1799747878793454e-07, unnormalized = 2.3811483156991327\n",
      "iteration 10, reconstruction error: 0.32496985086798114, decrease = 7.091635906286697e-08, unnormalized = 2.381147796074522\n",
      "iteration 11, reconstruction error: 0.3249698071824378, decrease = 4.368554334943653e-08, unnormalized = 2.3811474759779503\n",
      "iteration 12, reconstruction error: 0.3249697797475611, decrease = 2.7434876703757993e-08, unnormalized = 2.3811472749546985\n",
      "iteration 13, reconstruction error: 0.3249697622495713, decrease = 1.7497989779347023e-08, unnormalized = 2.381147146741882\n",
      "iteration 14, reconstruction error: 0.32496975094671793, decrease = 1.130285337547221e-08, unnormalized = 2.3811470639226164\n",
      "iteration 15, reconstruction error: 0.3249697435676326, decrease = 7.379085342762437e-09, unnormalized = 2.3811470098539305\n",
      "PARAFAC converged after 15 iterations\n",
      "Took 0 mins 17 secs\n",
      "peak memory: 147.15 MiB, increment: 17.60 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "start_time = time.time()\n",
    "sparse_kruskal = parafac_sparse(tensor, rank=rank, init='random', verbose=True)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print('Took %d mins %d secs' % (divmod(total_time, 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights, factors) : rank-5 KruskalTensor of shape (1000, 1001, 1002, 100) "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_kruskal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `factors_sparse` are sparse, we can reconstruct them into a tensor without using too much memory. In general, this will not be the case, but it is for our toy example. Let's do this to look at the absolute error for the decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.381147009853927"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(tensor - kruskal_to_tensor(sparse_kruskal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not actually necessary to compute this, as the same as the norm of the tensor times the reconstruction error that was printed by the algorithm (you can pass `return_errors=True` to `parafac()` to have the reconstruction errors be returned along with the factors). That is, $$\\mathrm{reconstruction\\ error} = \\frac{\\|\\mathrm{tensor} - \\mathrm{kruskal\\_to\\_tensor}(\\mathrm{factors})\\|_2}{\\|\\mathrm{tensor}\\|_2}$$ (they won't be exactly the same due to numerical differences in how they are calculated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3249697435676321"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(tensor - kruskal_to_tensor(sparse_kruskal))/tl.norm(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the nonzero entries to see how close it is to the original tensor. The factors satisfy $$\\sum_{r=0}^{R-1} {f_0}_r\\circ {f_1}_r \\circ {f_2}_r \\circ {f_3}_r,$$ where $R$ is the rank (here 5), ${f_i}_r$ is the $r$-th column of the $i$-th factor of the decomposition, and $\\circ$ is the vector outer product. Component-wise, this translates to a product of corresponding elements per component for each factor, summed over the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7,   7,   7, ..., 952, 952, 952],\n",
       "       [ 10,  10,  10, ..., 955, 955, 955],\n",
       "       [121, 260, 266, ..., 685, 810, 888],\n",
       "       [ 10,  10,  10, ...,  91,  91,  91]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00869538487621399"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_val = tensor[tuple(tensor.coords.T[0])]\n",
    "orig_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_sparse, factors_sparse = sparse_kruskal\n",
    "sparse_val = np.sum(np.prod(sparse.stack([factors_sparse[i][idx] for i, idx in enumerate(tuple(tensor.coords.T[0]))], 0), 0))\n",
    "sparse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00869538487621399"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(orig_val - sparse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here is mostly due to random chance. The total reconstruction errors for the two runs of algorithm were roughly the same. In general, the error of the factorization will vary due to the randomness of the initial factors chosen by the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
